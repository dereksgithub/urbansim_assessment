{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network Analysis\n",
    "### 1.1 Centrality Measures\n",
    "Select 3 centrality measures to characterise nodes, aiming at identifying the most important nodes in \n",
    "the underground network. Give the definition of each of the measures (including their equation), put \n",
    "the measures into the context of the underground, and why they will allow you to find the stations that \n",
    "are most crucial for the functioning of the underground. Compute the measures for your nodes in the \n",
    "network, and give the results in a table for the first 10 ranked nodes for each of the 3 measures.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "from operator import itemgetter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.1.1 Read the data, select centrality measures:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "london_graph = nx.read_graphml('london_tubenetwork.graphml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75\n",
      "to_directed_class\n",
      "__lt__\n",
      "copy\n",
      "graph\n",
      "name\n",
      "__le__\n",
      "__sizeof__\n",
      "graph\n",
      "__delattr__\n",
      "adjacency\n"
     ]
    }
   ],
   "source": [
    "print(len(dir(london_graph)))\n",
    "for i in np.random.randint(0, len(dir(london_graph)), 10):\n",
    "    print(dir(london_graph)[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "438\n",
      "486\n"
     ]
    }
   ],
   "source": [
    "print(london_graph.number_of_nodes())\n",
    "print(london_graph.number_of_edges())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([('940GZZLUKSX', 0.016018306636155607),\n",
       "  ('940GZZLUBST', 0.016018306636155607),\n",
       "  ('940GZZLUGPK', 0.013729977116704806),\n",
       "  ('940GZZLUOXC', 0.013729977116704806),\n",
       "  ('940GZZLUECT', 0.013729977116704806),\n",
       "  ('940GZZLUBNK', 0.013729977116704806),\n",
       "  ('940GZZLUWLO', 0.013729977116704806),\n",
       "  ('940GZZLUTNG', 0.011441647597254004),\n",
       "  ('940GZZLULVT', 0.011441647597254004),\n",
       "  ('940GZZLUWHM', 0.011441647597254004)],\n",
       " [('940GZZLUBST', 0.3810150084358616),\n",
       "  ('940GZZLUBLG', 0.3534325817535458),\n",
       "  ('940GZZLUFYR', 0.3365817857034552),\n",
       "  ('940GZZLUBNK', 0.3195625056858345),\n",
       "  ('940GZZLUGPK', 0.31955197127241747),\n",
       "  ('940GZZLUWLO', 0.3172160057103268),\n",
       "  ('940GZZLULVT', 0.3130260708612372),\n",
       "  ('940GZZLUWSM', 0.2899622285670293),\n",
       "  ('940GZZLUBND', 0.2585985889467725),\n",
       "  ('910GWHMDSTD', 0.23656559877955732)],\n",
       " [('940GZZLUGPK', 0.09489685124864278),\n",
       "  ('940GZZLUBND', 0.09373659373659374),\n",
       "  ('940GZZLUWSM', 0.09319684367669012),\n",
       "  ('940GZZLUBST', 0.09289965986394558),\n",
       "  ('940GZZLUWLO', 0.09238900634249471),\n",
       "  ('940GZZLUBNK', 0.092),\n",
       "  ('940GZZLUOXC', 0.09161425576519916),\n",
       "  ('940GZZLULVT', 0.09001029866117405),\n",
       "  ('940GZZLURGP', 0.08927477017364657),\n",
       "  ('940GZZLUFYR', 0.08916547643338094)])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Recompute the centrality measures\n",
    "degree_centrality = nx.degree_centrality(london_graph)\n",
    "betweenness_centrality = nx.betweenness_centrality(london_graph)\n",
    "closeness_centrality = nx.closeness_centrality(london_graph)\n",
    "\n",
    "# Sort and get the top 10 nodes for each centrality measure\n",
    "sorted_degree = sorted(degree_centrality.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "sorted_betweenness = sorted(betweenness_centrality.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "sorted_closeness = sorted(closeness_centrality.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "\n",
    "(sorted_degree, sorted_betweenness, sorted_closeness)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Delta Centrality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute initial betweenness centrality\n",
    "initial_betweenness = nx.betweenness_centrality(G)\n",
    "\n",
    "# Initialize a dictionary to store delta centrality\n",
    "delta_centrality = {}\n",
    "\n",
    "# Iterate over all nodes to compute delta centrality\n",
    "for node in G.nodes():\n",
    "    # Create a copy of the graph without the current node\n",
    "    G_copy = copy.deepcopy(G)\n",
    "    G_copy.remove_node(node)\n",
    "    \n",
    "    # Recompute betweenness centrality without the node\n",
    "    new_betweenness = nx.betweenness_centrality(G_copy)\n",
    "    \n",
    "    # Calculate the difference and store it\n",
    "    delta_centrality[node] = sum(abs(initial_betweenness[n] - new_betweenness.get(n, 0)) for n in initial_betweenness)\n",
    "\n",
    "# Sort nodes by their delta centrality\n",
    "sorted_delta_centrality = sorted(delta_centrality.items(), key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Delta centrality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import copy\n",
    "\n",
    "# Load your network\n",
    "G = nx.read_graphml('path_to_your_graphml_file.graphml')\n",
    "\n",
    "# Compute initial betweenness centrality\n",
    "initial_betweenness = nx.betweenness_centrality(G)\n",
    "\n",
    "# Initialize a dictionary to store delta centrality\n",
    "delta_centrality = {}\n",
    "\n",
    "# Iterate over all nodes to compute delta centrality\n",
    "for node in G.nodes():\n",
    "    # Create a copy of the graph without the current node\n",
    "    G_copy = copy.deepcopy(G)\n",
    "    G_copy.remove_node(node)\n",
    "    \n",
    "    # Recompute betweenness centrality without the node\n",
    "    new_betweenness = nx.betweenness_centrality(G_copy)\n",
    "    \n",
    "    # Calculate the difference and store it\n",
    "    delta_centrality[node] = sum(abs(initial_betweenness[n] - new_betweenness.get(n, 0)) for n in initial_betweenness)\n",
    "\n",
    "# Sort nodes by their delta centrality\n",
    "sorted_delta_centrality = sorted(delta_centrality.items(), key=lambda x: x[1], reverse=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find 2 different measures to evaluate the impact of the node removal on the network. These need to \n",
    "be global measures referring to the whole network and not to specific nodes or links. Explain whether \n",
    "these two measures are specific to the London underground, or whether they could also be used to \n",
    "evaluate the resilience of any other network.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each of the centrality measures selected in I.1. remove at least 10 nodes following two different \n",
    "strategies. A) Non-sequential removal: using the table created in I.1. remove 1 node at a time \n",
    "following the rank in the table, i.e. from the most important one to the 10\n",
    "\n",
    "th most important one. After \n",
    "each removal, evaluate the impact of the removal using your two measures in I.2. and proceed until \n",
    "you have removed at least 10 nodes. B) Sequential: remove the highest ranked node and evaluate the \n",
    "impact using the 2 measures. After removal, re-compute the centrality measure. Remove the highest \n",
    "ranked node in the new network and evaluate the impact. Continue until removing at least 10 nodes. \n",
    "\n",
    "Report the results of the 2 strategies in one plot, and critically discuss the following: which centrality \n",
    "measure reflects better the importance of a station for the functioning of the underground, which \n",
    "strategy is more effective at studying resilience, and which impact measure is better at assessing the \n",
    "damage after node removal.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
